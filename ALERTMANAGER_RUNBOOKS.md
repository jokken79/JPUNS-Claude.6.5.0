# üö® JPUNS Alertmanager Runbooks - Procedimientos de Respuesta

**Versi√≥n**: 1.0
**Fecha**: 2025-11-22
**Estado**: ‚úÖ Gu√≠a de respuesta a alertas

---

## üìã √çndice de Runbooks

1. [Alertas de API](#api-alerts)
2. [Alertas de Base de Datos](#database-alerts)
3. [Alertas de Cache](#cache-alerts)
4. [Alertas de Sistema](#system-alerts)
5. [Alertas de Yukyu](#yukyu-alerts)

---

## üî¥ API Alerts {#api-alerts}

### Alert: Backend API No Disponible (CR√çTICO)

**S√≠ntomas:**
- Alert "BackendAPIDown" activo en Alertmanager
- Grafana muestra 0 requests
- Usuarios reportan "Connection refused"

**Pasos de Respuesta:**

#### Paso 1: Verificar estatus (1 minuto)
```bash
# Verifica si el servicio est√° corriendo
curl -v http://localhost:8000/health

# Ver logs
docker logs jpuns-backend | tail -50

# Ver estado del contenedor
docker ps | grep jpuns-backend
```

**Si ves "Connection refused":**
‚Üí Contin√∫a con Paso 2

**Si ves respuesta 500:**
‚Üí Contin√∫a con Paso 3

#### Paso 2: Reiniciar servicio backend (2 minutos)
```bash
# Det√©n el servicio
docker-compose stop jpuns-backend

# Espera 5 segundos
sleep 5

# Inicia nuevamente
docker-compose up -d jpuns-backend

# Verifica que est√° healthy
sleep 10
curl http://localhost:8000/health
```

**Resultado esperado:**
```json
{"status": "healthy", "timestamp": "2025-11-22T..."}
```

#### Paso 3: Investigar logs de error (5 minutos)
```bash
# Ver √∫ltimos 100 l√≠neas
docker logs jpuns-backend | tail -100

# Buscar errores espec√≠ficos
docker logs jpuns-backend | grep -i "error\|exception\|fatal"

# Si es error de base de datos, contin√∫a con Database Alerts
# Si es error de memoria, contin√∫a con System Alerts
```

#### Paso 4: Rollback si es necesario (5 minutos)
```bash
# Si el problema es c√≥digo nuevo, revierte al √∫ltimo commit bueno
git log --oneline -5
git checkout <previous-good-commit>

# Reconstruye la imagen
docker-compose build jpuns-backend

# Reinicia
docker-compose up -d jpuns-backend

# Verifica
curl http://localhost:8000/health
```

#### Paso 5: Escalar si persiste (notifica a lead)
```bash
# Informaci√≥n a compartir:
echo "=== INFORMACI√ìN DE ESCALADA ==="
docker ps jpuns-backend
docker logs jpuns-backend --tail 50
curl -s http://localhost:8000/health | python -m json.tool
ps aux | grep -i backend
```

**Thresholds de Escalada:**
- Si no responde despu√©s de 2 reintentos ‚Üí Escalar a Tech Lead
- Si hay error de base de datos ‚Üí Verificar Database Alerts
- Si hay OOM (Out of Memory) ‚Üí Verificar System Alerts

---

### Alert: Tasa de Error Alta (> 5%) - CR√çTICO

**S√≠ntomas:**
- Alert "HighErrorRate" activada
- Grafana muestra pico en 5xx responses
- Algunos endpoints respondiendo con error

**Pasos de Respuesta:**

#### Paso 1: Identificar endpoints afectados (2 minutos)
```bash
# En Prometheus (http://localhost:9090):
# Ejecuta esta query:
topk(10, rate(http_requests_total{status=~"5.."}[5m]))
```

**Esto mostrar√° qu√© endpoints generan m√°s errores**

#### Paso 2: Verificar logs para errores (3 minutos)
```bash
# Busca errores en logs
docker logs jpuns-backend --since 5m | grep -i "error\|exception" | head -20

# Si es error de database:
# ‚Üí Ir a Database Alerts section

# Si es error de validaci√≥n:
# ‚Üí Revisar datos de entrada

# Si es timeout:
# ‚Üí Contin√∫a con Paso 3
```

#### Paso 3: Verificar salud de servicios dependientes (2 minutos)
```bash
# Verifica PostgreSQL
docker exec jpuns-postgres-monitoring pg_isready -U postgres

# Verifica Redis
redis-cli PING

# Verifica conectividad entre servicios
docker exec jpuns-backend curl -v http://jpuns-postgres-monitoring:5432
```

#### Paso 4: Analizar patrones de error (5 minutos)
```bash
# Busca patr√≥n de error espec√≠fico
docker logs jpuns-backend --since 10m | grep "ERROR" | cut -d' ' -f5-10 | sort | uniq -c | sort -rn

# Ejemplo salida:
#     15 JSONDecodeError: field validation error
#      8 ConnectionRefusedError: Redis not responding
#      3 OutOfMemoryError: heap space
```

#### Paso 5: Acciones correctivas espec√≠ficas

**Si es error de validaci√≥n:**
```bash
# Estos errores generalmente son por datos inv√°lidos de cliente
# Revisar logs de error detallados
docker logs jpuns-backend -f

# Posible soluci√≥n: Verificar schema de API
# Si es un cambio reciente, revertir
```

**Si es timeout:**
```bash
# Aumentar timeout temporalmente
# En docker-compose.yml, busca:
#   timeout: 10s
# Cambia a:
#   timeout: 30s

docker-compose down
docker-compose up -d

# Monitorea si se resuelve:
curl -s http://localhost:9090/api/v1/query?query=rate\(http_requests_total\{status~%225..%22\}\[5m\]\) | python -m json.tool
```

**Si es problema de base de datos:**
‚Üí Ir a Database Alerts section

**Si es falta de recursos:**
‚Üí Ir a System Alerts section

---

### Alert: Tiempo de Respuesta Lento (> 1s) - WARNING

**S√≠ntomas:**
- Alert "SlowAPIResponse" activada
- Usuarios reportan que el dashboard carga lentamente
- P95 response time > 1 segundo

**Pasos de Respuesta:**

#### Paso 1: Verificar si es problema de cache (2 minutos)
```bash
# Ver hit rate de cache
redis-cli INFO stats | grep -E "hits|misses"

# Si hit rate es bajo, cache est√° fallando:
docker logs jpuns-redis-monitoring | tail -20
```

#### Paso 2: Identificar endpoint lento (3 minutos)
```bash
# En Prometheus, ejecuta:
topk(5, histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])))
```

**Esto mostrar√° los 5 endpoints m√°s lentos**

#### Paso 3: Analizar query del endpoint lento (5 minutos)
```bash
# Ejemplo: /api/dashboard/yukyu-trends-monthly es lento

# Verifica si hay queries lentas en DB
docker exec jpuns-postgres-monitoring \
  psql -U postgres -d jpuns_production \
  -c "SELECT query, calls, mean_time FROM pg_stat_statements WHERE mean_time > 100 ORDER BY mean_time DESC LIMIT 5;"

# Si ves query lenta:
# ‚Üí Ir a Database Alerts section
```

#### Paso 4: Acciones de optimizaci√≥n r√°pida

**Opci√≥n A: Borrar cache para recargar (1 minuto)**
```bash
# Borra cache de endpoint espec√≠fico
redis-cli DEL "cache:yukyu:*"

# Verifica que se vuelve a popular
curl -s http://localhost:8000/api/dashboard/yukyu-trends-monthly \
  -H "Authorization: Bearer <token>" | python -m json.tool
```

**Opci√≥n B: Escalar recursos temporalmente (5 minutos)**
```bash
# En docker-compose.yml, aumenta recursos:
services:
  jpuns-backend:
    deploy:
      resources:
        limits:
          cpus: '2.0'      # Cambiar de 1.0 a 2.0
          memory: 1024M    # Cambiar de 512M a 1024M

docker-compose up -d jpuns-backend
```

**Opci√≥n C: Reducir timeout de request (2 minutos)**
```bash
# A veces requests lentas se pueden cancelar m√°s r√°pido
# En endpoint, cambiar:
#   response_timeout = 10  # A
#   response_timeout = 5

# Esto devuelve error m√°s r√°pido vs esperar
```

---

## üóÑÔ∏è Database Alerts {#database-alerts}

### Alert: Base de Datos No Disponible (CR√çTICO)

**S√≠ntomas:**
- Alert "PostgreSQLDown" activada
- Todos los endpoints retornan error 500
- "Connection refused" en logs

**Pasos de Respuesta:**

#### Paso 1: Verificar conectividad (1 minuto)
```bash
# Test de conectividad
docker exec jpuns-postgres-monitoring pg_isready -U postgres

# Output esperado: "accepting connections"
# Si no: "rejecting connections"
```

#### Paso 2: Iniciar la base de datos (2 minutos)
```bash
# Verifica si el contenedor est√° corriendo
docker ps | grep postgres

# Si no est√° corriendo:
docker-compose up -d jpuns-postgres-monitoring

# Espera 10 segundos para iniciar
sleep 10

# Verifica nuevamente
docker exec jpuns-postgres-monitoring pg_isready -U postgres
```

#### Paso 3: Verificar logs de error (3 minutos)
```bash
# Ve logs
docker logs jpuns-postgres-monitoring | tail -50 | grep -i "error\|fatal"

# Errores comunes:
# "out of disk space" ‚Üí Limpia datos viejos (ver System Alerts)
# "shared memory exhausted" ‚Üí Reinicia servidor
# "connection limit reached" ‚Üí Ver conexiones activas
```

#### Paso 4: Reiniciar base de datos (3 minutos)
```bash
# Pausa servicios que usan la DB
docker-compose stop jpuns-backend jpuns-frontend

# Reinicia la base de datos
docker-compose restart jpuns-postgres-monitoring

# Espera a que inicie
sleep 15

# Verifica
docker exec jpuns-postgres-monitoring pg_isready -U postgres

# Reinicia servicios
docker-compose up -d jpuns-backend jpuns-frontend
```

#### Paso 5: Verificar integridad de datos (5 minutos)
```bash
# Conexi√≥n directa
docker exec -it jpuns-postgres-monitoring psql -U postgres -d jpuns_production

# En psql:
SELECT version();  -- Verifica que est√° online
SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public';  -- Verifica tablas

# Si alguna tabla est√° da√±ada:
REINDEX INDEX index_name;
```

---

### Alert: Conexiones DB Altas (> 80%) - WARNING

**S√≠ntomas:**
- Alert "HighDatabaseConnections" activada
- Dashboard lento
- Nuevas conexiones se rechazan

**Pasos de Respuesta:**

#### Paso 1: Ver conexiones activas (1 minuto)
```bash
# En Prometheus:
pg_stat_activity_count

# O en psql:
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT count(*) as connection_count FROM pg_stat_activity;"
```

#### Paso 2: Identificar conexiones inactivas (2 minutos)
```bash
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT pid, usename, state, query, query_start FROM pg_stat_activity WHERE state = 'idle' ORDER BY query_start DESC LIMIT 10;"
```

#### Paso 3: Terminar conexiones inactivas (2 minutos)
```bash
# CUIDADO: Verifica que son realmente inactivas

docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'idle' AND query_start < now() - interval '1 hour';"
```

#### Paso 4: Aumentar l√≠mite de conexiones (5 minutos)
```bash
# En docker-compose.yml:
jpuns-postgres-monitoring:
  environment:
    - POSTGRES_INIT_ARGS=-c max_connections=200  # Cambiar de 100 a 200

docker-compose down
docker-compose up -d jpuns-postgres-monitoring
```

---

### Alert: Queries Lentas (> 1s) - WARNING

**S√≠ntomas:**
- Alert "SlowDatabaseQueries" activada
- Algunos endpoints lentos
- CPU de DB alta

**Pasos de Respuesta:**

#### Paso 1: Identificar queries lentas (2 minutos)
```bash
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT query, calls, mean_time, max_time FROM pg_stat_statements WHERE mean_time > 1000 ORDER BY mean_time DESC LIMIT 10;"
```

#### Paso 2: Analizar plan de ejecuci√≥n (3 minutos)
```bash
# Para la query m√°s lenta, obt√©n el plan:
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "EXPLAIN (ANALYZE, BUFFERS) SELECT ... FROM ..."
```

**Busca PROBLEMAS:**
- Sequential Scan (debe ser Index Scan)
- High buffer reads
- High execution time

#### Paso 3: Crear √≠ndice si es necesario (5 minutos)
```bash
# Ejemplo: Query hace sequential scan en tabla grande
# Soluci√≥n: Crear √≠ndice

docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "CREATE INDEX idx_table_column ON table_name(column_name);"

# Verifica que se cre√≥
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT indexname FROM pg_indexes WHERE tablename = 'table_name';"
```

#### Paso 4: Resetear estad√≠sticas (1 minuto)
```bash
# Despu√©s de cambios, resetea para poder ver mejora
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT pg_stat_statements_reset();"
```

---

## üíæ Cache Alerts {#cache-alerts}

### Alert: Redis No Disponible (CR√çTICO)

**S√≠ntomas:**
- Alert "RedisDown" activada
- Todos los requests van sin cache (lentos)
- "Connection refused" para cache

**Pasos de Respuesta:**

#### Paso 1: Verificar conectividad (1 minuto)
```bash
redis-cli PING

# Resultado esperado: "PONG"
# Si no: error de conexi√≥n
```

#### Paso 2: Iniciar Redis (2 minutos)
```bash
# Verifica si est√° corriendo
docker ps | grep redis

# Si no:
docker-compose up -d jpuns-redis-monitoring

# Espera 5 segundos
sleep 5

# Verifica
redis-cli PING
```

#### Paso 3: Limpiar datos corruptos (3 minutos)
```bash
# Si PING falla pero contenedor est√° corriendo:

# Conecta a Redis
redis-cli

# Verifica integridad
INFO server

# Si hay problemas, limpia cache:
FLUSHALL  # Borra TODO el cache (solo si est√° OK)

# Exit
exit
```

#### Paso 4: Reiniciar si persiste (2 minutos)
```bash
docker-compose restart jpuns-redis-monitoring
sleep 5
redis-cli PING
```

---

### Alert: Hit Rate de Cache Bajo (< 30%) - WARNING

**S√≠ntomas:**
- Alert "LowCacheHitRate" activada
- APIs lentas aunque funciona
- Muchas requests a database

**Pasos de Respuesta:**

#### Paso 1: Ver estad√≠sticas de cache (1 minuto)
```bash
redis-cli INFO stats

# Busca:
# - hits: N√∫mero de cache hits
# - misses: N√∫mero de cache misses
# - hit_rate: hits / (hits + misses)
```

#### Paso 2: Analizar patrones de acceso (3 minutos)
```bash
# Ver keys m√°s accedidas
redis-cli --scan | head -20

# Ver tama√±o de cache
redis-cli INFO memory | grep used_memory_human

# Ejemplo:
# If used_memory < 10MB y hit_rate bajo ‚Üí cache vac√≠o
# If used_memory > 500MB y hit_rate bajo ‚Üí cache inefectivo
```

#### Paso 3: Calentar cache si est√° vac√≠o (2 minutos)
```bash
# Opci√≥n A: Llamar endpoint principal para popular cache
curl -s http://localhost:8000/api/dashboard/yukyu-trends-monthly \
  -H "Authorization: Bearer <token>" > /dev/null

# Opci√≥n B: Scripts de pre-warming
python backend/scripts/warm_cache.py
```

#### Paso 4: Aumentar TTL de cache si expira muy r√°pido (3 minutos)
```bash
# En backend code, busca:
#   cache_ttl = 3600  # 1 hora
# Cambia a:
#   cache_ttl = 7200  # 2 horas

# Reconstruye:
docker-compose build jpuns-backend
docker-compose up -d jpuns-backend

# Verifica hit rate despu√©s de 5 minutos
redis-cli INFO stats | grep hits
```

---

## üñ•Ô∏è System Alerts {#system-alerts}

### Alert: CPU Muy Alta (> 95%) - CR√çTICO

**S√≠ntomas:**
- Alert "CriticalCPUUsage" activada
- Sistema responde lentamente
- M√∫ltiples servicios lentos

**Pasos de Respuesta:**

#### Paso 1: Identificar qu√© proceso usa CPU (2 minutos)
```bash
# Ver top en tiempo real
top -bn1 | head -20

# O ver por contenedor
docker stats --no-stream

# Busca el que usa > 95%
```

#### Paso 2: Si es backend
```bash
# Logs para ver qu√© est√° procesando
docker logs jpuns-backend -f

# Si ves loop infinito o proceso loco:
docker-compose restart jpuns-backend

# Monitorea
docker stats jpuns-backend
```

#### Paso 3: Si es database
```bash
# Ver query que usa CPU
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT pid, query, query_start FROM pg_stat_activity ORDER BY query_start DESC LIMIT 5;"

# Termina query problem√°tica si es necesario:
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE pid = <PID>;"
```

#### Paso 4: Escalar recursos (5 minutos)
```bash
# En docker-compose.yml:
jpuns-backend:
  deploy:
    resources:
      limits:
        cpus: '2.0'  # Aumenta de 1.0 a 2.0

docker-compose up -d jpuns-backend
```

---

### Alert: Memoria Muy Alta (> 95%) - CR√çTICO

**S√≠ntomas:**
- Alert "CriticalMemoryUsage" activada
- Sistema empieza a usar swap
- Aplicaciones mueren por OOM

**Pasos de Respuesta:**

#### Paso 1: Identificar qu√© usa memoria (2 minutos)
```bash
docker stats --no-stream

# Busca contenedor con > 95% memory
```

#### Paso 2: Analizar memory leak (3 minutos)
```bash
# Si backend:
docker stats jpuns-backend --no-stream

# Monitorea por 5 minutos:
for i in {1..5}; do docker stats jpuns-backend --no-stream; sleep 60; done

# Si memoria crece constantemente ‚Üí memory leak
# Si memoria estable ‚Üí normal high usage
```

#### Paso 3: Liberar memoria temporalmente (2 minutos)
```bash
# Opci√≥n A: Limpiar cache
redis-cli FLUSHALL

# Opci√≥n B: Reiniciar servicio
docker-compose restart jpuns-backend

# Ver cambio:
docker stats jpuns-backend --no-stream
```

#### Paso 4: Escalar o investigar (5 minutos)
```bash
# Si es memory leak, investigar c√≥digo
git log --oneline -10
git diff HEAD~1

# Si es volumen de datos normal:
# Aumentar memoria en docker-compose.yml:
jpuns-backend:
  deploy:
    resources:
      limits:
        memory: 1024M  # Cambiar de 512M a 1024M
```

---

### Alert: Disco Bajo (< 15% libre) - WARNING

**S√≠ntomas:**
- Alert "LowDiskSpace" activada
- Base de datos podr√≠a dejar de escribir
- Logs se detienen

**Pasos de Respuesta:**

#### Paso 1: Ver uso de disco (1 minuto)
```bash
df -h

# Busca particiones con < 15% disponible
```

#### Paso 2: Identificar qu√© ocupa espacio (2 minutos)
```bash
# Ver carpetas grandes
du -sh /* | sort -rh | head -10

# En espec√≠fico, logs:
du -sh /var/lib/docker/volumes

# Base de datos:
docker exec jpuns-postgres-monitoring \
  du -sh /var/lib/postgresql/data
```

#### Paso 3: Limpiar datos viejos (5 minutos)
```bash
# Opci√≥n A: Limpiar logs viejos
docker logs jpuns-backend > /dev/null  # Trunca logs

# Opci√≥n B: Reducir retenci√≥n de Prometheus
# En docker-compose.yml, edita Prometheus:
#   - '--storage.tsdb.retention.time=7d'  # Cambiar de 30d

docker-compose up -d jpuns-prometheus

# Opci√≥n C: Limpiar base de datos
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "DELETE FROM logs WHERE created_at < now() - interval '7 days';"
```

#### Paso 4: Monitorear liberaci√≥n (2 minutos)
```bash
watch df -h

# Espera a que Prometheus compacte (puede tomar 5-10 min)
```

---

## üìä Yukyu Alerts {#yukyu-alerts}

### Alert: Endpoint Yukyu No Responde - CR√çTICO

**S√≠ntomas:**
- Alert "YukyuEndpointDown" activada
- Dashboard no carga datos de Yukyu
- "/api/dashboard/yukyu-*" retorna error

**Pasos de Respuesta:**

#### Paso 1: Verificar endpoint directamente (1 minuto)
```bash
# Test endpoint
curl -v http://localhost:8000/api/dashboard/yukyu-trends-monthly \
  -H "Authorization: Bearer <token>"

# Si 503: Backend no disponible ‚Üí Ver API Alerts
# Si 401: Token inv√°lido ‚Üí Genera nuevo token
# Si 404: Endpoint no existe ‚Üí Verificar endpoint name
# Si 500: Error de l√≥gica ‚Üí Ver logs
```

#### Paso 2: Verificar datos en database (2 minutos)
```bash
# Conecta a DB
docker exec -it jpuns-postgres-monitoring psql -U postgres -d jpuns_production

# Verifica tabla de yukyu
SELECT count(*) FROM yukyu_requests;
SELECT count(*) FROM yukyu_approvals;

# Si count = 0, hay problema de datos
```

#### Paso 3: Limpiar cache de Yukyu (1 minuto)
```bash
# Borra cache del endpoint espec√≠fico
redis-cli DEL "cache:yukyu:trends:*"
redis-cli DEL "cache:yukyu:compliance:*"

# Llamar endpoint nuevamente
curl http://localhost:8000/api/dashboard/yukyu-trends-monthly \
  -H "Authorization: Bearer <token>"
```

#### Paso 4: Verificar permisos (2 minutos)
```bash
# Si 403 Forbidden, verifica rol del usuario
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "SELECT role FROM users WHERE user_id = '<user-id>';"

# Debe ser 'KEITOSAN' o 'KANRININSHA'
```

---

### Alert: Tiempo de Respuesta de Yukyu Lento - WARNING

**S√≠ntomas:**
- Alert "SlowYukyuResponse" activada
- Dashboard tarda en cargar
- P95 > 1000ms

**Pasos de Respuesta:**

#### Paso 1: Ver tiempo en Prometheus (2 minutos)
```promql
# Query en Prometheus:
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{path="/api/dashboard/yukyu-trends-monthly"}[5m]))
```

#### Paso 2: Verificar cache hit (2 minutos)
```bash
redis-cli INFO stats

# Si hits bajo ‚Üí cache miss
# Soluci√≥n: Calentar cache
curl http://localhost:8000/api/dashboard/yukyu-trends-monthly

# Llamar nuevamente, debe ser m√°s r√°pido
```

#### Paso 3: Analizar query de fiscal year (3 minutos)
```bash
# El endpoint calcula fiscal year, que puede ser lento
# Verifica tiempo de query:

docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "EXPLAIN ANALYZE SELECT * FROM yukyu_requests WHERE fiscal_year = 2024;"

# Si es lento, crear √≠ndice:
docker exec jpuns-postgres-monitoring psql -U postgres -d jpuns_production -c \
  "CREATE INDEX idx_yukyu_fiscal_year ON yukyu_requests(fiscal_year);"
```

---

## üìû Escalada de Alertas

### Escalation Policy

```
‚è±Ô∏è 5 min: Alert dispara, equipo notificado v√≠a Slack
‚è±Ô∏è 10 min: Si no hay respuesta, mensaje a on-call engineer
‚è±Ô∏è 15 min: Si no hay respuesta, llamada al Tech Lead
‚è±Ô∏è 20 min: M√°xima prioridad, todos notificados
```

### C√≥mo Escalar

```bash
# 1. En Slack, agrega el thread
# 2. Menciona @on-call
# 3. Proporciona:
#    - Alert name
#    - Current value
#    - Pasos ya realizados
#    - Logs relevantes

# Ejemplo:
# @on-call: HighErrorRate alerta
# Current: 8.5% (threshold 5%)
# Pasos realizados: Reiniciado backend, verified DB connectivity
# Logs: /tmp/escalation-logs.txt
```

---

## ‚úÖ Checklist de Post-Resoluci√≥n

- [ ] Alert resuelto y Prometheus muestra OK
- [ ] Servicios respondiendo correctamente
- [ ] Logs limpios (sin errores nuevos)
- [ ] Documentar root cause en Slack
- [ ] Crear ticket para prevenir repetici√≥n si es necesario
- [ ] Actualizar runbook si el procedimiento cambi√≥

---

**Documento Version**: 1.0
**√öltima Actualizaci√≥n**: 2025-11-22
**Estado**: ‚úÖ RUNBOOKS COMPLETOS Y LISTOS
